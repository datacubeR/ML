---
title: 'Tarea 2: KNN'
author: "Alex Alvarez y Alfonso Tobar"
date: "2022-10-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción 

El siguiente reporte tiene como objetivo analizar el algoritmo de KNN para la predicción del diámetros de asteroides.  Para resolver este problema se nos hizo entrega de dos dataset. Uno de entrenamiento de 100.000 observaciones y 26 features, mientras que para el set de testeo tenemos 37.681 observaciones.

El objetivo final es encontrar el modelo que entregue la mejor performance en el set de testeo, el cual no posee etiquetas. 

```{r}
set.seed(42) # Reproducibilidad
pacman::p_load(tidyverse, magrittr, umap, tidymodels, kknn)

df_train <- read_csv("dataTrain.csv") %>% 
  select(-...1, -index)
df_test <- read_csv("dataEval.csv") %>% 
  select(-...1)
```

```{r}
df_train %>% 
  glimpse()
```

> Es posible observar que de las 26 variables, la mayoría son de tipo numérico. Se tiene que las variables `spec_B`, `spec_T`, `neo`, `pha` son de tipo categórico y la variable full_name es el nombre asociado a cada asteroide.
# EDA

## Vector Objetivo

```{r}
df_train %>% 
  ggplot(aes(x = diameter)) +
  geom_histogram(fill = "green", color = "black") +
  ggtitle('Distribución del Diámetro') + 
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
df_train %>% 
  ggplot(aes(x = log(diameter))) +
  geom_histogram(fill = "blue", color = "black") +
  ggtitle('Distribución del Log del Diámetro') + 
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
df_train %>% 
  select_if(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 10)+
  facet_wrap(~key, scales = "free_x")
```

## Chequear los valores Nulos

```{r}
df_train %>% 
  select_if(is.numeric) %>% 
  summarise_each(funs(sum(is.na(.)))) %>% 
  gather() %>% 
  mutate(value = value/nrow(df_train)) %>% 
  arrange(desc(value)) %>% 
  ggplot(aes(x = reorder(key, -value), y = value)) + 
  geom_col(fill = "purple" ,color = "black") +
  scale_y_continuous(labels=scales::percent) +
  ggtitle('Distribución de Valores Nulos en Variables Continuas') + 
  theme(plot.title = element_text(hjust = 0.5), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

```{r}
df_train %>% 
  filter(is.na(diameter))
```

```{r}

df_train %>% 
  select_if(negate(is.numeric)) %>% 
  summarise_each(funs(sum(is.na(.)))) %>% 
  gather() %>% 
  mutate(value = value/nrow(df_train)) %>% 
  arrange(desc(value)) %>% 
  ggplot(aes(x = reorder(key, -value), y = value)) + 
  geom_col(fill = "yellow" ,color = "black") +
  scale_y_continuous(labels=scales::percent) +
  ggtitle('Distribución de Valores Nulos en Variables Categóricas') + 
  theme(plot.title = element_text(hjust = 0.5), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```
```{r}
df_train %>% 
  ggplot(aes(x = neo)) + geom_bar(fill = "blue", color = "black") +
  ggtitle('Distribución de Valores Nulos en Variables Categóricas') + 
  theme(plot.title = element_text(hjust = 0.5), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```
```{r}
df_train %>% 
  ggplot(aes(x = pha)) + geom_bar(fill = "orange", color = "black") +
  ggtitle('Distribución de Valores Nulos en Variables Categóricas') + 
  theme(plot.title = element_text(hjust = 0.5), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

```{r}
to_remove <- df_train %>% 
  summarise_each(funs(mean(is.na(.)))) %>% 
  gather() %>% 
  filter(value > 0.6) %>% 
  pull(key)
to_remove
```

# Correlaciones

```{r}
df_train[setdiff(colnames(df_train),to_remove)] %>% 
  select_if(is.numeric) %>% 
  drop_na() %>% 
  cor() %>% 
  heatmap()
```


```{r}
corr_mat <- df_train[setdiff(colnames(df_train),to_remove)] %>% 
  select_if(is.numeric) %>% 
  drop_na() %>% 
  cor() %>% 
  data.frame() 
corr_mat
corr_mat["diameter"] %>%
  arrange(desc(diameter))
```

```{r}
remove_corr = c("i","per_y","om","w","e","condition_code")
```

```{r}
quantile(df_train$diameter, probs = c(0, 0.01,0.05,0.1,0.25,0.5, 0.75, 0.9, 0.95, 0.99, 1), na.rm = TRUE)
```


# Modelamiento



Caso 1


```{r}
mean_list <- df_train %>% 
  select_if(is.numeric) %>% 
  summarise_each(funs(mean(., na.rm = TRUE))) %>% 
  as.list()
```
```{r}
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
```

```{r}
mode_list <- df_train %>% 
  select_if(negate(is.numeric)) %>% 
  summarise_each(funs(getmode(.))) %>% 
  as.list()
  
```

```{r}
library(fastDummies)
train_set <- df_train[,!(names(df_train) %in% c(to_remove,remove_corr, "full_name"))] %>% 
  drop_na(diameter) %>% 
  mutate(neo = factor(neo),
         pha = factor(pha)
         ) 
  # summarise_each(funs(sum(is.na(.))))
```

```{r}
data_split <- initial_split(train_set, prop = 3/4)

train_data <- training(data_split)
val_data  <- testing(data_split)
```

```{r}
train_data_prep <- train_data %>% 
  replace_na(mean_list) %>%
  replace_na(mode_list) %>% 
  dummy_cols() %>% 
  select(-neo, -pha)

val_data_prep <- val_data %>% 
  replace_na(mean_list) %>%
  replace_na(mode_list) %>% 
  dummy_cols() %>% 
  select(-neo, -pha) %>% 
  scale()
  
means <- train_data_prep %>% 
  summarise_each(funs(mean(.))) 
sds <- train_data_prep %>% 
  summarise_each(funs(sd(.))) 


```


```{r}
for (i in 1:ncol(train_data_prep)){
  train_data_prep[i] <- (train_data_prep[i]-as.numeric(means[i]))/as.numeric(sds[i])
  val_data_prep[i] <- (val_data_prep[i]-as.numeric(means[i]))/as.numeric(sds[i])
}
val_data_prep <- val_data_prep %>% as.data.frame()

```


```{r}
train_knn <- function(n){
  knn_mod <- nearest_neighbor(neighbors = n) %>%
  set_mode("regression") %>%
  set_engine("kknn")  %>% 
  fit(diameter ~ ., data = train_data_prep)
y_pred <- predict(knn_mod, val_data_prep)


return(rmse_vec(
  truth = val_data_prep$diameter, 
  estimate = y_pred$.pred
))
}
```




```{r}
# rmse_3 <- train_knn(3)
# rmse_5 <- train_knn(5)
# rmse_7 <- train_knn(7)
# rmse_9 <- train_knn(9)
# rmse_11 <- train_knn(11)
# rmse_13 <- train_knn(13)
# rmse_15 <- train_knn(15)
# rmse_17 <- train_knn(17)
# rmse_19 <- train_knn(19)
# rmse_21 <- train_knn(21)
```



# Conclusiones
